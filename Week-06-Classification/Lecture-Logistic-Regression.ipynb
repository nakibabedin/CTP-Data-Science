{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Machine Learning.\n",
    "\n",
    "# \"ISBE\"  The Motto and Main Steps when building a Machine Learning Model. \n",
    "## 1. I - Inspect and explore data.\n",
    "## 2. S - Select and engineer features.\n",
    "## 3. B - Build and train model.\n",
    "## 4. E - Evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries \n",
    "\n",
    "# Pandas and numpy for data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn / matplotlib for visualization \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Helper function to split our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is our Logit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Helper fuctions to evaluate our model.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# Display data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary\n",
    "<img src='https://miro.medium.com/max/1260/1*rr3UGlpEv_PSMc1pyqa4Uw.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect and Explore EDA\n",
    "1. Shape and size\n",
    "1. Describe\n",
    "1. Info\n",
    "1. Check for nulls\n",
    "1. Check for dupes\n",
    "1. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shape and size\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get info on cols\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect null values.\n",
    "* What does this tell us about features we should and should not use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect / check for nulls.\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of null values per columns\n",
    "((df.isnull().sum() / len(df)) * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect duplicate rows.\n",
    "* Phew..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dupes = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows are %i.\" % n_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='survived');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "sns.kdeplot(data=df, x='age', hue='survived', shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sex')['survived'].sum() / df.groupby('sex')['survived'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"S\" Select and Engineer Features\n",
    "1. Select the features you are going to want to use to predict survived. \n",
    "    * For this first example we are only going to be selecting `fare, sex, and pclass`\n",
    "    * Don't use features that have nulls in them. \n",
    "1. Convert categorical variables into numerical. \n",
    "    * Use helper function `pd.get_dummies()` for this \n",
    "1. Split into test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical variables into numerical.\n",
    "* `pd.get_dummies()` is a very helpful function that converts our categorical variables into continuous variables. \n",
    "* have to be careful about the ['dummy variable trap'](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) which leads to multicollinearity problems which we just dont have time to discuss, [to learn more watch this](https://www.youtube.com/watch?v=Cba9LJ9lS8s&ab_channel=zedstatistics).  git  \n",
    "* more about [dummy variable traps](https://medium.com/nerd-for-tech/what-is-dummy-variable-trap-how-it-can-be-handled-using-python-78ec17246331)\n",
    "* Removing one of the dummy variable columns solves this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['sex', 'pclass'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['sex', 'pclass'], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['fare', 'pclass_2', 'pclass_3', 'sex_male']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our X and y\n",
    "### y is what we are trying to predict, and X is what we are using to make that prediction.\n",
    "* It is industry standard to name your feature matrix as `X`, and your target variable as `y`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['fare', 'pclass_2', 'pclass_3', 'sex_male']\n",
    "\n",
    "X = df[selected_features]\n",
    "\n",
    "y = df['survived']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data into training and testing batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "print('Lenght of our Training data:', X_train.shape, '\\nLength of our Testing data:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"B\" - Build and train our model\n",
    "* Initalize an empty model\n",
    "* Train our model using our `model.fit()` with our training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initalize our model.  \n",
    "# This will create an empty untrained Logistic Regression model.\n",
    "model = LogisticRegression()\n",
    "\n",
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the heart of our ML process. \n",
    "This steps fits (aka trains) our model with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! THIS HAPPENS 'IN PLACE', MEANING IT DOESN'T RETURN ANYTHING !!\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new predicitions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "y_pred_proba.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame.from_dict( \n",
    "    {'y_true': y_test, \n",
    "     'y_pred': y_pred, \n",
    "     'probability': y_pred_proba} )\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E = Evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, our first look.\n",
    "Is the percent of predicitions we got correct.\n",
    "Good for general scoring, but bad in terms of when classes are imbalanced. \n",
    "\n",
    "It is the count of all the predictions you got correct divided by the total number of predictions.\n",
    "Aka, Percent of predictions we got correct.\n",
    "\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper fuctions to evaluate our model. \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Accuracy Score: 0.826816\n",
    "\n",
    "print(\"Accuracy Score: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "Out of all the times the MODEL says 'yes' what was the precentage it was correct. \n",
    "* The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative. \n",
    "* The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "* If you want to raise precision (ie; only say yes when you are absolutely sure), raise your classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision Score: %f\" % precision)\n",
    "print(\"In other words, when the model predicts someone survived, it is correct %f of the time.\" % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall\n",
    "Out of all the times the ACTUAL is 'yes', how many did you get correct.  \n",
    "\n",
    "Having high recall is important when the cost of missing a True Positive is high.  \n",
    "\n",
    "For example, if you're detecting cancer.  Saying, you don't have cancer when you really do is really bad. Therefore, if you're building a model to detect cancer, you should optimize for having high recall.  You can do that by lowering your classification threshold. \n",
    "\n",
    "* The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "* The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "* if you want to raise recall, lower your classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall Score: %f\" % recall)\n",
    "print(\"In other words, it correctly identifies %f percent of all survivors\" % recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score\n",
    "The ‘harmonic mean’ of precision and recall. \n",
    "\n",
    "Good for an overall evaluation metric. \n",
    "\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# F1 Score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confustion Matrix\n",
    "    * True Positive (TP) - When you say yes and actual is yes.\n",
    "    * False Positive (FP) - When you say yes and actual is no.\n",
    "    * False Negative (FN) - When you say no and actual is yes.\n",
    "    * True Negative (TN) - When you say no and actual is no. \n",
    "\n",
    "The first term (True or False) is if the prediction was correct or not. True means correct, False means incorrect.\n",
    "The second term (Positive or Negative) is what the classifier guessed.  Did it say it Yes, or did it say No. \n",
    "\n",
    "True Positive (TP): A true positive is an outcome where the model correctly predicts the positive class. When we say YES survived, actual is YES survived. Having a high True-Positive rate is GOOD.\n",
    "\n",
    "True Negative (TN): A true negative is an outcome where the model correctly predicts the negative class.  When we say NO survived, actual is NO survived. Having a high true-negative rate is GOOD.\n",
    "\n",
    "False Negative (FN):  A false negative is an outcome where the model incorrectly predicts the negative class. We say NO survived, actual is YES survived.\n",
    "Having a high False-Negative rate is BAD.\n",
    "\n",
    "False Positive (FP):  A false positive is an outcome where the model incorrectly predicts the positive class. We say YES survived, actual is NO survived.\n",
    "Having a high False-Positive rate is BAD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Just rounding them so the numbers are easier to read\n",
    "cm = cm.round(2)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Greens', fmt='g')\n",
    "plt.title(\"Confusion Matrix of Titanic Suvivors\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# print('true-negitive:', tn, \n",
    "#       '\\nfalse-positive:', fp, \n",
    "#       '\\nfalse-negative:', fn, \n",
    "#       '\\ntrue-positive:', tp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix \n",
    "\n",
    "# ACCURACY\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score: %f\" % accuracy)\n",
    "\n",
    "# F1 ACCURACY\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score: %f\" % f1)\n",
    "\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Greens', fmt='g')\n",
    "plt.ylabel('Ground Truth')\n",
    "plt.xlabel('Model Prediction');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
